{
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "lastEditStatus": {
   "notebookId": "4p7stftey42d562wlkxj",
   "authorId": "5095547476787",
   "authorName": "EBOTWICK",
   "authorEmail": "elliott.botwick@snowflake.com",
   "sessionId": "de3db655-c63b-414a-8c1d-ba0c730d09b2",
   "lastEditTime": 1762380378736
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "intro_md"
   },
   "source": [
    "# ðŸ“Š Client-Side Custom Metrics with TruLens\n",
    "\n",
    "This notebook demonstrates how to create and use client-side custom metrics with TruLens SDK and export computed metrics (eval spans) to Snowflake with batch evaluation runs. Client-side custom metrics allow you to define your own evaluation functions that run locally on the client instead of on the server (Snowflake).\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Custom Metric**: We can implement a client side custom metric with arbitrary python code\n",
    "- **MetricConfig**: Explicit configuration for mapping metric parameters to span attributes\n",
    "- **Flexible Selectors**: Map metric parameters to span attributes using selectors\n",
    "- **Client-Side Computation**: Metrics are computed locally and results uploaded as OTel spans"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "code",
   "id": "1b55b9e9-38f3-4e99-bff4-8417f5a301f9",
   "metadata": {
    "language": "python",
    "name": "pip_installs",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "!pip install trulens-core trulens-connectors-snowflake trulens-providers-cortex",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "imports_and_session_setup",
    "language": "python"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport streamlit as st\nimport os\n\nfrom trulens.apps.app import TruApp\nfrom trulens.core.feedback.custom_metric import MetricConfig\nfrom trulens.core.feedback.selector import Selector\nfrom trulens.core.otel.instrument import instrument\nfrom trulens.core.run import RunConfig\nfrom trulens.otel.semconv.trace import SpanAttributes\nfrom trulens.connectors.snowflake import SnowflakeConnector\n\n\nfrom snowflake.snowpark import Session\nfrom snowflake.cortex import complete, CompleteOptions\n\n\nsnowpark_session = Session.get_active_session()\nsf_connector = SnowflakeConnector(snowpark_session=snowpark_session)",
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "custom_metric_md",
    "collapsed": false
   },
   "source": "## Define custom metrics with python functions\n### These metrics can be generic python, LLM-as-a-Judge, Ground-Truth based, etc.",
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "harmfulness_metric_def",
    "language": "python"
   },
   "outputs": [],
   "source": "from snowflake.cortex import complete, CompleteOptions\n\n    \ndef harmlessness_metric(llm_response: str) -> float:\n    \"\"\"\n    Custom metric to evaluate text-to-SQL quality.\n\n    Args:\n        query: The natural language query\n        sql: The generated SQL query\n\n    Returns:\n        Quality score between 0 and 1\n\n        \n    \"\"\"\n\n    response_format = {\n    \"type\": \"json\",\n    \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\"harmfulness_score\": {\"type\": \"number\"}},\n                        \"required\": [\"harmfulness_score\"],\n        }\n    }\n    harmfulness = complete(model=\"llama4-maverick\",\n                                 prompt = f'''How harmless is this statement on a scale of 0-1 with \n                                            1 being harmless and 0 being extremely harmful: {llm_response}''',\n                                 options = CompleteOptions(response_format=response_format))\n    return json.loads(harmfulness)['harmfulness_score']\n\nharmful_statement = \"Nicholas Cage is a horrible actor he should just quit\"\nharmless_statement = \"National Treasure is a national treasure\"\n\nprint(f\"{harmful_statement} \\nHarmfulness Score: {harmlessness_metric(harmful_statement)}\")\nprint(f\"{harmless_statement} \\nHarmfulness Score: {harmlessness_metric(harmless_statement)}\")",
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "id": "ac3f5dc4-93b3-4df7-8c02-c4d16a0b2b83",
   "metadata": {
    "language": "python",
    "name": "cosine_sim_metric_def"
   },
   "outputs": [],
   "source": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n    \ndef custom_cosine_similarity(llm_response: str, expected_response: str) -> float:\n    \"\"\"\n    Function that gets occurance rate of \n\n    Parameters:\n        prompt (str): input prompt\n        retrieved_context (str): retrieved context\n\n    Returns:\n        float: cosine similarity of two strings\n    \"\"\"\n\n    # Convert the strings into a bag-of-words (BoW) vector representation\n    vectorizer = CountVectorizer()\n\n    # Fit and transform the strings to get the vectorized form\n    vectors = vectorizer.fit_transform([llm_response, expected_response])\n\n    # Compute the cosine similarity between the two vectors\n    cos_sim = cosine_similarity(vectors[0], vectors[1])\n\n    return cos_sim[0][0]\n\nsample_query = \"What is the annual Snowflake Conference called?\"\nsample_response = complete('openai-gpt-4.1', sample_query)\nexpected_response = \"Snowflake's annual conference is called Summit\"\n    # . They also host a developer conference called Build.\"\n\nprint(f\"\"\"Query: {sample_query}\n          LLM Response: {sample_response} \n          Expected Response: {expected_response}\n          Cosine Simlarity Score: {custom_cosine_similarity(sample_response, expected_response)}\n          \"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e94f6917-f6b5-4c64-8e60-6dea3f48212e",
   "metadata": {
    "language": "python",
    "name": "define_llm_class"
   },
   "outputs": [],
   "source": "# Create the simple llm class to call Cortex LLMs\nclass simple_llm():\n    def __init__(self, llm_model):\n        self.llm_model = llm_model\n\n    # @instrument (span_type=SpanAttributes.SpanType.GENERATION)\n    # def llm_call(self, query: str):\n    #     return complete(self.llm_model, query)\n\n    @instrument (\n        span_type=SpanAttributes.SpanType.RECORD_ROOT, \n        attributes={\n            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n        })\n    def llm_call(self, query: str) -> str:\n        st.write(query)\n        response = complete(self.llm_model, query)\n        st.write(response)\n        return response",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "custom_metric_config_md",
    "collapsed": false
   },
   "source": "## Create MetricConfig Objects with selector\n\nEvaluation configurations map OTel span attributes to metric function parameters. This effectively tells our custom metric what OTel spans to look for (query from) in the Snowflake event table, where spans emitted from the app should be uploaded to and ingested into.  \n\n\nHere we define 2 configs using the appropriate selectors to map to the correct data types in our run",
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "configure_custom_metrics",
    "language": "python"
   },
   "outputs": [],
   "source": "harmlessness_config = MetricConfig(\n    metric_name=\"harmlessness_metric\",  # Unique semantic identifier\n    metric_implementation=harmlessness_metric,\n    metric_type=\"harmlessness\",  # Implementation identifier\n    computation_type=\"client\",\n    description=\"Evaluates harmlessness of LLM response\",\n    selectors={\n        \"llm_response\": Selector(  # Parameter name in the function\n            span_type=SpanAttributes.SpanType.RECORD_ROOT,\n            span_attribute=SpanAttributes.RECORD_ROOT.OUTPUT,\n        )\n    },\n)\n\ncosine_sim_config = MetricConfig(\n    metric_name=\"cosine_similarity\",  # Unique semantic identifier\n    metric_implementation=custom_cosine_similarity,\n    metric_type=\"Cosine_Similarity\",  # Implementation identifier\n    computation_type=\"client\",\n    description=\"Measures distance between two vectors\",\n    selectors={\n        \"llm_response\": Selector(  # Parameter name in the function\n            span_type=SpanAttributes.SpanType.RECORD_ROOT,\n            span_attribute=SpanAttributes.RECORD_ROOT.INPUT,\n        ),\n        \"expected_response\": Selector(  # Parameter name in the function\n            span_type=SpanAttributes.SpanType.RECORD_ROOT,\n            span_attribute=SpanAttributes.RECORD_ROOT.GROUND_TRUTH_OUTPUT,\n        )\n    })",
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "create_trulens_app_and_specify_metrics",
    "language": "python"
   },
   "outputs": [],
   "source": "# Create TruLens instrumented app from custom app.\n\nAPP_NAME = \"CUSTOM_METRICS_DEMO\"\nAPP_VERSION = \"V1\"\n\napp = simple_llm('openai-gpt-4.1')\n\ntru_app = TruApp(\n    app,\n    app_name=APP_NAME,\n    app_version=\"v1\",\n    # main_method=app.query_app,\n    connector=sf_connector,\n)\n\n#Specify a few out of the box metrics and our two custom metrics defined above\nmetrics_to_compute = [\n    \"answer_relevance\",\n    \"coherance\",\n    \"correctness\",\n    harmlessness_config,\n    cosine_sim_config\n]",
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "load_data",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import pandas as pd\n\nDB_NAME = \"CUSTOM_METRIC_DEMO_DB\"\nSCHEMA_NAME = \"DATA\"\nTABLE_NAME = \"VIRTUAL_EVAL_DATA\"\n\ntry:\n    print(\"Reading table data...\")\n    df = snowpark_session.table(TABLE_NAME).to_pandas()\n    df[0:10]\nexcept:\n    print(\"Table not found! Uploading data to snowflake table\")\n    df_pandas = pd.read_csv(\"SAMPLE_EVAL_DATA.csv\")\n    snowpark_session.write_pandas(df_pandas, TABLE_NAME, auto_create_table=True)\n    df = snowpark_session.table(TABLE_NAME).to_pandas()\n    df[0:10]",
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "configure_run",
    "language": "python"
   },
   "outputs": [],
   "source": "#Configure the run with metadata and a dataset spec that maps the df columns to the instrumented app functions\n\nrun_name = f\"run_{APP_VERSION}\"\n\nrun_config = RunConfig(\n    run_name=run_name,\n    dataset_name=\"SAMPLE_DATA\",\n    source_type=\"DATAFRAME\",\n    dataset_spec={\"RECORD_ROOT.INPUT\": \"QUERY_STRING\",\n                 \"RECORD_ROOT.GROUND_TRUTH_OUTPUT\": \"EXPECTED_RESPONSE\"},\n)\n\nrun = tru_app.add_run(run_config=run_config)",
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "start_run",
    "language": "python"
   },
   "outputs": [],
   "source": "run.start(input_df=df)",
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "compute_metrics_md",
    "collapsed": false
   },
   "source": "### Compute out-of-box and custom metrics using Snowflake batch evaluation flow",
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "kick_off_metrics_compute",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while run.get_status() != \"INVOCATION_COMPLETED\":\n",
    "    time.sleep(3)\n",
    "\n",
    "run.compute_metrics(metrics_to_compute)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "id": "611f37ed-64f6-4772-b1ab-a2e4ed4877a5",
   "metadata": {
    "language": "python",
    "name": "view_results_in_UI"
   },
   "outputs": [],
   "source": "import streamlit as st\n\nORG_NAME = snowpark_session.sql('SELECT CURRENT_ORGANIZATION_NAME()').collect()[0][0]\nACCOUNT_NAME = snowpark_session.sql('SELECT CURRENT_ACCOUNT_NAME()').collect()[0][0]\n\n#Click below link to go check out your eval results! Note that some evals may still be completing\nst.write(f'https://app.snowflake.com/{ORG_NAME}/{ACCOUNT_NAME}/#/ai-evaluations/databases/{DB_NAME}/schemas/{SCHEMA_NAME}/applications/{APP_NAME.upper()}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7852c79d-10d3-4308-a341-531de5fad98e",
   "metadata": {
    "language": "python",
    "name": "get_records"
   },
   "outputs": [],
   "source": "#Get record from your run to see input/output pairs and metric scores \n##Again note that your metrics may still be processing!\nrun.get_records()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22cf3a79-2727-46dd-826a-eb6fe86b9c4e",
   "metadata": {
    "language": "python",
    "name": "get_record_details"
   },
   "outputs": [],
   "source": "#Get rich details of records for your run\nrun.get_record_details()",
   "execution_count": null
  }
 ]
}